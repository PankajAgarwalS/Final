# -*- coding: utf-8 -*-
"""letterDL2.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ju7D2a_2TFTKTttgOT_vuXW0Yil2dHUl

## Import Libraries
"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd

"""## Loading Dataset"""

data_path = 'letter-recognition.data'
columns = ['letter'] + [f'feature_{i}' for i in range(16)]
df = pd.read_csv(data_path, names=columns)

label_encoder = LabelEncoder()
df['target'] = label_encoder.fit_transform(df['letter'])
X = df.drop(['letter', 'target'], axis=1)
y = df['target']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Model Fitting"""

# Build the deep neural network model
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(16,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(26, activation='softmax'))  # 26 classes for letters

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""## Predicting"""

# Given data
import numpy as np
new_data = np.array([4, 7, 5, 5, 4, 6, 7, 3, 7, 11, 8, 9, 3, 8, 4, 8]).reshape(1, -1)

# Use the model to make predictions
predictions = model.predict(new_data)
print(predictions)
# Display the predictions
predicted_class = np.argmax(predictions)
print(f'The predicted class is: {predicted_class}')

class_mapping = {
    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J',
    10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T',
    20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'
}

# Display the predicted class using the mapping
predicted_letter = class_mapping[predicted_class]
print(f'The predicted class is: {predicted_class}, which corresponds to the letter: {predicted_letter}')

"""## Dynamic Input"""

#dummy = 4, 7, 5, 5, 4, 6, 7, 3, 7, 11, 8, 9, 3, 8, 4, 8

# Take input from the user
user_input = input("Enter values for the 16 features separated by commas: ")
user_input_list = [int(x) for x in user_input.split(',')]

# Convert the user input to a NumPy array
new_data = np.array(user_input_list).reshape(1, -1)

# Use the model to make predictions
predictions = model.predict(new_data)
#print(predictions)
# Display the predictions
predicted_class = np.argmax(predictions)
print(f'\nThe predicted class is: {predicted_class} i.e. {class_mapping[predicted_class]}')

"""Multiclass Classification for OCR Letter Recognition with Deep Neural Networks
This code demonstrates how to use a Deep Neural Network (DNN) to perform multiclass classification on the Optical Character Recognition (OCR) Letter Recognition dataset. The goal is to train a model that can accurately identify which letter of the alphabet (A-Z) is represented by a given set of features extracted from a character image.

1. The OCR Letter Recognition Dataset

The dataset, obtained from the UCI Machine Learning Repository, contains a collection of image features extracted from letter images.

Each data point represents a single letter.

The features describe various characteristics of the letter image, such as its shape, size, and orientation.

The task is a multiclass classification problem because there are 26 possible classes, corresponding to the 26 letters of the English alphabet.

2. Deep Neural Networks (DNNs) for Multiclass Classification

DNNs are a type of artificial neural network with multiple layers between the input and output layers.

Their ability to learn complex, hierarchical representations from data makes them well-suited for tasks like image recognition and classification.

For multiclass classification, the DNN is designed to output a probability distribution over all possible classes. The class with the highest probability is then chosen as the predicted class.

3. Code Explanation

Import Libraries:

tensorflow and keras: These are used to build and train the DNN model. Keras provides a high-level API for defining neural network architectures, while TensorFlow is the underlying framework for numerical computation.

pandas: This library is used for data manipulation, specifically loading the dataset from a CSV file and organizing it into a DataFrame.

sklearn.model_selection.train_test_split: This function is used to split the dataset into training and testing sets.

sklearn.preprocessing.LabelEncoder: This class is used to convert the letter labels (A-Z) into numerical values (0-25), which is required for training the neural network.

Loading and Preprocessing the Dataset:

The code loads the letter recognition dataset from the 'letter-recognition.data' file using pd.read_csv().  It assigns names to the columns.

A LabelEncoder is used to transform the categorical letter labels ('A', 'B', 'C', ..., 'Z') into numerical labels (0, 1, 2, ..., 25). This is necessary because neural networks work with numerical data.

The dataset is then split into features (X) and target labels (y). The 'letter' and 'target' columns are dropped from X, as they contain the original letter labels and the encoded numerical labels, respectively.

The data is further divided into training and testing sets using train_test_split(). This allows the model to be trained on one subset of the data and evaluated on a separate, unseen subset, providing an estimate of its generalization performance.

Building the DNN Model:

A Sequential model is created using Keras. This allows you to define the layers of the neural network in a linear stack.

The model consists of the following layers:

Input Layer: A Dense layer with 128 neurons and ReLU (Rectified Linear Unit) activation. The input_shape=(16,) specifies that each data point has 16 features.

Hidden Layer: Another Dense layer with 64 neurons and ReLU activation. Hidden layers learn intermediate representations of the input data.

Output Layer: A Dense layer with 26 neurons (one for each letter) and softmax activation. The softmax function converts the layer's output into a probability distribution over the 26 classes.

Compiling the Model:

The model is compiled using the compile() method. This specifies:

optimizer='adam': The Adam optimizer is used to update the model's weights during training.

loss='sparse_categorical_crossentropy': This loss function is suitable for multiclass classification problems where the labels are integers.

metrics=['accuracy']: The model's performance is evaluated using accuracy, which measures the proportion of correctly classified letters.

Training the Model:

The model is trained using the fit() method.

X_train and y_train: The training data and labels are passed to the model.

epochs=10: The model iterates through the entire training dataset 10 times.

batch_size=32: The training data is processed in batches of 32 samples at a time.

validation_data=(X_test, y_test): The model's performance on the test set is evaluated after each epoch, which helps to monitor for overfitting.

Evaluating the Model:

The trained model is evaluated on the test set using the evaluate() method. This returns the test loss and test accuracy, providing an estimate of how well the model generalizes to unseen data.

Making Predictions:

The code demonstrates how to use the trained model to predict the letter corresponding to new data.

The predict() method is used to generate a probability distribution over the 26 letters for the input data.

np.argmax() is used to find the index of the class with the highest probability, which corresponds to the predicted letter.

A dictionary class_mapping maps the numerical class labels back to their corresponding letter.

Dynamic Input:

The code includes a section to take user input for a single data point.

The user is prompted to enter 16 comma-separated values representing the features of a letter.

This input is converted into a NumPy array and reshaped to match the expected input format of the model.

The model then predicts the letter for this user-provided input, and the result is displayed.

4. Key Concepts

Multiclass Classification: Classifying data points into one of several (more than two) classes.

Optical Character Recognition (OCR): The task of automatically recognizing characters from images.

Deep Neural Networks (DNNs): Neural networks with multiple hidden layers, capable of learning complex patterns.

Feature Extraction: The process of extracting relevant features from raw data (in this case, letter images) that are useful for classification.

Label Encoding: Converting categorical labels (letters) into numerical values.

Softmax Activation: A function used in the output layer of a DNN for multiclass classification, which converts the raw output into a probability distribution over the classes.

Sparse Categorical Crossentropy: A loss function used for multiclass classification with integer labels.

Optimizer: An algorithm (like Adam) used to update the model's weights during training to minimize the loss function.

Epoch: One complete pass through the entire training dataset during training.

Batch Size: The number of samples processed before the model's weights are updated.

Training and Testing Sets: Splitting the data into training and testing sets to evaluate the model's generalization performance.

Accuracy: A metric used to evaluate the model's performance, measuring the proportion of correctly classified samples.

This code provides a basic example of how to use a DNN for OCR letter recognition. With further experimentation, such as adding more layers, tuning hyperparameters, and using more advanced techniques, the accuracy of the model can be improved.
"""