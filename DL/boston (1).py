# -*- coding: utf-8 -*-
"""Boston.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fNUYYFlDv7JI8ODTyro-Uz70g8O_DnLU

## Importing libraries
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Activation
from tensorflow.keras.datasets import boston_housing
from tensorflow.keras import layers

import tensorflow as tf
import matplotlib.pyplot as plt

SEED_VALUE = 42

# Fix seed to make training deterministic.
np.random.seed(SEED_VALUE)
tf.random.set_seed(SEED_VALUE)

"""## Loading dataset"""

# Load the Boston housing dataset.
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

print(X_train.shape)
print("\n")
print("Input features: ", X_train[0])
print("\n")
print("Output target: ", y_train[0])

boston_features = {
    'Average Number of Rooms':5,
}

X_train_1d = X_train[:, boston_features['Average Number of Rooms']]
print(X_train_1d.shape)

X_test_1d = X_test[:, boston_features['Average Number of Rooms']]

"""## Visualizations"""

plt.figure(figsize=(15, 5))

plt.xlabel('Average Number of Rooms')
plt.ylabel('Median Price [$K]')
plt.grid("on")
plt.scatter(X_train_1d[:], y_train, color='green', alpha=0.5);

"""## Model Building"""

model = Sequential()

# Define the model consisting of a single neuron.
model.add(Dense(units=1, input_shape=(1,)))

# Display a summary of the model architecture.
model.summary()

model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=.005), loss='mse')

"""## Model Fitting"""

history = model.fit(X_train_1d,
                    y_train,
                    batch_size=16,
                    epochs=101,
                    validation_split=0.3)

"""## Validation"""

def plot_loss(history):
    plt.figure(figsize=(20,5))
    plt.plot(history.history['loss'], 'g', label='Training Loss')
    plt.plot(history.history['val_loss'], 'b', label='Validation Loss')
    plt.xlim([0, 100])
    plt.ylim([0, 300])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

plot_loss(history)

"""## Prediction"""

x = np.array([[3], [4], [5], [6], [7]])  # shape: (5, 1)
y_pred = model.predict(x)

for idx in range(len(x)):
    print("Predicted price of a home with {} rooms: ${}K".format(x[idx][0], int(y_pred[idx]*10)/10))

# Generate feature data that spans the range of interest for the independent variable.
x = tf.linspace(3, 9, 10)

# Use the model to predict the dependent variable.
y = model.predict(x)

"""## Conclusion"""

def plot_data(x_data, y_data, x, y, title=None):
    plt.figure(figsize=(15,5))
    plt.scatter(x_data, y_data, label='Ground Truth', color='green', alpha=0.5)
    plt.plot(x, y, color='k', label='Model Predictions')
    plt.xlim([3,9])
    plt.ylim([0,60])
    plt.xlabel('Average Number of Rooms')
    plt.ylabel('Price [$K]')
    plt.title(title)
    plt.grid(True)
    plt.legend()

plot_data(X_train_1d, y_train, x, y, title='Training Dataset')

plot_data(X_test_1d, y_test, x, y, title='Test Dataset')

"""1. The Basic Linear Regression Neuron

A single neuron in a neural network performs a weighted sum of its inputs and adds a bias term.  This is precisely the calculation done in linear regression.

Mathematically:  output = (w1 * x1) + (w2 * x2) + ... + (wn * xn) + b

x1, x2, ..., xn are the input features.

w1, w2, ..., wn are the weights associated with each input feature.

b is the bias term.

2. Representing Linear Regression with a Neural Network

To implement linear regression with a neural network, you create a network with a single layer containing one neuron.

Crucially, this output neuron has no activation function.  The direct output of the weighted sum and bias is used as the prediction.

3. Why Use a Deep Neural Network for Linear Regression?

At first, it seems counterintuitive to use a deep neural network (a network with multiple layers) for linear regression, which is inherently a simple linear model.  However, there are a few reasons and nuances:

Feature Learning (with extra layers): If you add hidden layers before the final output layer, the initial layers can learn complex, non-linear combinations of the input features.  Even though the final layer performs a linear transformation, the preceding layers can transform the data into a higher-level representation that might improve the linear fit. In the corrected code, the layers:

Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Hidden layer 1
Dense(32, activation='relu'),  # Hidden layer 2

act as feature extractors.

Regularization: Deep neural networks have techniques like dropout and weight decay that can act as regularizers, potentially preventing overfitting, even in a linear regression context.  The relu activation in the hidden layers also contributes to non-linearity and can help with regularization.

Framework Familiarity: Using a neural network framework like TensorFlow/Keras for linear regression can provide familiarity with the framework, which is essential for more complex deep learning tasks.

Generalization: In some cases, especially with high-dimensional data, a deep network with non-linear hidden layers might learn a more robust representation of the data, leading to better generalization than a simple linear model.  This is because the hidden layers can learn feature combinations that a simple linear model cannot.

4. Training the Network

The neural network is trained using an optimization algorithm (like Adam) to minimize a loss function.  For linear regression, the typical loss function is Mean Squared Error (MSE).

The optimizer adjusts the weights and biases of the network to reduce the MSE between the predicted and actual values.

5. Application to Boston Housing Data

In the Boston Housing Price prediction problem, the goal is to predict the median value of owner-occupied homes based on features like crime rate, number of rooms, etc.

By using a deep neural network with a final linear output layer, the model can learn complex relationships between these features and the house prices, potentially leading to more accurate predictions than a simple linear regression model that directly relates the original features to the output.

The hidden layers learn intermediate representations of the data. For example, one hidden neuron might learn a feature that represents "overall neighborhood quality," which is a combination of several input features.

Key Points

The final layer in a neural network performing linear regression has a single neuron and no activation function.

While a single-layer network can do basic linear regression, using a deep network with non-linear activation functions in the hidden layers allows the network to learn complex feature representations, potentially improving the linear fit in the output layer.
"""

