{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "# Get dynamic input from user for the size of vectors\n",
        "N = int(input(\"Enter the size of the vectors: \"))\n",
        "\n",
        "# Uncomment this block to allow dynamic input for vector values\n",
        "# A = cp.array([int(input(f\"Enter value for A[{i}]: \")) for i in range(N)], dtype=cp.int32)\n",
        "# B = cp.array([int(input(f\"Enter value for B[{i}]: \")) for i in range(N)], dtype=cp.int32)\n",
        "\n",
        "# Comment this block if you want to provide manual input for the vectors\n",
        "A = cp.random.randint(0, 10, size=N, dtype=cp.int32)\n",
        "B = cp.random.randint(0, 10, size=N, dtype=cp.int32)\n",
        "\n",
        "# Create an empty array C to store the result\n",
        "C = cp.empty_like(A)\n",
        "\n",
        "# CUDA kernel code to add vectors A and B\n",
        "kernel_code = '''\n",
        "extern \"C\" __global__\n",
        "void add(const int* A, const int* B, int* C, int size) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        C[tid] = A[tid] + B[tid];\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile the kernel code using CuPy's RawModule\n",
        "module = cp.RawModule(code=kernel_code)\n",
        "add_kernel = module.get_function(\"add\")\n",
        "\n",
        "# Calculate number of blocks and threads per block\n",
        "threads_per_block = 256\n",
        "blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "# Execute the kernel to add vectors\n",
        "add_kernel((blocks_per_grid,), (threads_per_block,), (A, B, C, N))\n",
        "\n",
        "# Print the result\n",
        "print(\"Vector A:\", A.get())\n",
        "print(\"Vector B:\", B.get())\n",
        "print(\"Addition:\", C.get())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O5klKcexPdP",
        "outputId": "0468541d-51d9-4d77-bde5-5c4bef2533b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector A: [6 2 3 2]\n",
            "Vector B: [0 6 8 4]\n",
            "Addition: [ 6  8 11  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the code is to add two integer vectors A and B of size N, and store the result in vector C using GPU parallelism via CUDA.\n",
        "\n",
        "âš™ï¸ Basic Working\n",
        "Memory Allocation (Host and Device):\n",
        "\n",
        "Host memory: A, B, C are normal C++ arrays on the CPU.\n",
        "\n",
        "Device memory: X, Y, Z are pointers allocated on the GPU using cudaMalloc.\n",
        "\n",
        "Data Transfer (Host â‡Œ Device):\n",
        "\n",
        "The contents of A and B are copied from CPU to GPU memory using cudaMemcpy.\n",
        "\n",
        "Kernel Launch:\n",
        "\n",
        "The function add<<<blocks, threads>>>(...) is a CUDA kernel. It's called from the CPU but runs in parallel on the GPU.\n",
        "\n",
        "Each thread is responsible for computing one element of the result: C[i] = A[i] + B[i].\n",
        "\n",
        "Parallelism:\n",
        "\n",
        "Threads are grouped into blocks, and blocks are grouped into a grid.\n",
        "\n",
        "In your code:\n",
        "\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "This gives each thread a unique index (tid) to process one element of the vector.\n",
        "\n",
        "Result Copy and Cleanup:\n",
        "\n",
        "Once the GPU finishes, the result in Z is copied back to C on the CPU.\n",
        "\n",
        "All dynamically allocated memory is freed.\n",
        "\n",
        "ðŸ“¦ CUDA Concepts Used\n",
        "1. Kernel Function\n",
        "A special function prefixed with __global__ that runs on the GPU.\n",
        "\n",
        "Launched from host code using triple angle brackets <<<...>>>.\n",
        "\n",
        "2. Thread Hierarchy\n",
        "threadIdx.x: Thread index within a block.\n",
        "\n",
        "blockIdx.x: Block index within a grid.\n",
        "\n",
        "blockDim.x: Number of threads per block.\n",
        "\n",
        "Final thread ID = blockIdx.x * blockDim.x + threadIdx.x.\n",
        "\n",
        "3. Memory Types\n",
        "Host memory: Allocated with new or malloc.\n",
        "\n",
        "Device memory: Allocated with cudaMalloc.\n",
        "\n",
        "Transfers:\n",
        "\n",
        "CPU â†’ GPU: cudaMemcpy(..., cudaMemcpyHostToDevice)\n",
        "\n",
        "GPU â†’ CPU: cudaMemcpy(..., cudaMemcpyDeviceToHost)\n",
        "\n",
        "4. Synchronization\n",
        "cudaDeviceSynchronize() ensures all GPU operations are complete.\n",
        "\n",
        "Optional but important for correctness when timing or debugging.\n",
        "\n",
        "ðŸš€ Why Use CUDA Here?\n",
        "Parallelism: Instead of looping on CPU, we launch many threads on GPU that execute simultaneously.\n",
        "\n",
        "Speed-up: For large arrays, this is significantly faster.\n",
        "\n"
      ],
      "metadata": {
        "id": "SrwBi3QbyiDI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFAGNDJwxeD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}